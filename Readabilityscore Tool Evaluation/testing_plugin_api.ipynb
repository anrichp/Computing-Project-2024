{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "import statistics\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "class MoodleReadabilityAPI:\n",
    "    def __init__(self, base_url: str, token: str):\n",
    "        self.base_url = base_url\n",
    "        self.token = token\n",
    "        self.endpoint = f\"{self.base_url}/webservice/rest/server.php\"\n",
    "\n",
    "    def calculate_readability(self, texts: List[str], urls: List[str]) -> List[Dict[str, Any]]:\n",
    "        if len(texts) != len(urls):\n",
    "            raise ValueError(\"The number of texts and URLs must match\")\n",
    "\n",
    "        results = []\n",
    "        for text, url in zip(texts, urls):\n",
    "            params = {\n",
    "                'wstoken': self.token,\n",
    "                'wsfunction': 'block_readabilityscore_process_text',\n",
    "                'moodlewsrestformat': 'json',\n",
    "                'selectedtext': text,\n",
    "                'pageurl': url\n",
    "            }\n",
    "\n",
    "            try:\n",
    "                response = requests.post(self.endpoint, data=params)\n",
    "                response.raise_for_status()\n",
    "                result = response.json()\n",
    "                results.append(result)\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                raise\n",
    "\n",
    "        return results\n",
    "\n",
    "def read_corpus(base_dir: str) -> List[Dict[str, Any]]:\n",
    "    corpus_dir = Path(base_dir) / \"Texts-SeparatedByReadingLevel\"\n",
    "    if not corpus_dir.is_dir():\n",
    "        raise FileNotFoundError(f\"Corpus directory not found: {corpus_dir}\")\n",
    "\n",
    "    corpus = []\n",
    "    level_dirs = {'Ele-Txt': 'elementary', 'Int-Txt': 'intermediate', 'Adv-Txt': 'advanced'}\n",
    "    \n",
    "    for dir_name, level in level_dirs.items():\n",
    "        level_dir = corpus_dir / dir_name\n",
    "        if not level_dir.is_dir():\n",
    "            continue\n",
    "        \n",
    "        for file_path in level_dir.glob('*.txt'):\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read().strip()\n",
    "                corpus.append({\n",
    "                    'text': text,\n",
    "                    'level': level,\n",
    "                    'file': file_path.name\n",
    "                })\n",
    "    \n",
    "    return corpus\n",
    "\n",
    "def classify_score(score: float) -> str:\n",
    "    if score <= 9:\n",
    "        return 'elementary'\n",
    "    elif score <= 13:\n",
    "        return 'intermediate'\n",
    "    else:\n",
    "        return 'advanced'\n",
    "\n",
    "def calculate_confidence(score: float) -> float:\n",
    "    if score <= 8.5 or score >= 13.5:\n",
    "        return 1.0\n",
    "    elif 8.5 < score <= 9.5 or 12.5 < score <= 13.5:\n",
    "        return 0.7\n",
    "    else:\n",
    "        return 0.5\n",
    "\n",
    "def validate_accuracy(api: MoodleReadabilityAPI, corpus: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    texts = [item['text'] for item in corpus]\n",
    "    urls = [f\"http://example.com/{item['file']}\" for item in corpus]\n",
    "    results = api.calculate_readability(texts, urls)\n",
    "\n",
    "    plugin_scores = [result['readabilityscore'] for result in results]\n",
    "    corpus_levels = [item['level'] for item in corpus]\n",
    "\n",
    "    level_map = {'elementary': 1, 'intermediate': 2, 'advanced': 3}\n",
    "    mapped_levels = [level_map[classify_score(score)] for score in plugin_scores]\n",
    "    \n",
    "    correlation, _ = pearsonr(plugin_scores, mapped_levels)\n",
    "\n",
    "    correct_classifications = sum(1 for result, item in zip(results, corpus) if classify_score(result['readabilityscore']) == item['level'])\n",
    "    accuracy = correct_classifications / len(corpus)\n",
    "\n",
    "    level_accuracy = {}\n",
    "    level_confusion = {level: Counter() for level in ['elementary', 'intermediate', 'advanced']}\n",
    "    detailed_results = []\n",
    "\n",
    "    for item, result in zip(corpus, results):\n",
    "        score = result['readabilityscore']\n",
    "        predicted_level = classify_score(score)\n",
    "        confidence = calculate_confidence(score)\n",
    "        \n",
    "        level_confusion[item['level']][predicted_level] += 1\n",
    "        \n",
    "        detailed_results.append({\n",
    "            'file': item['file'],\n",
    "            'actual_level': item['level'],\n",
    "            'predicted_level': predicted_level,\n",
    "            'gunning_fog_index': score,\n",
    "            'confidence': confidence,\n",
    "            'continuous_scale': score\n",
    "        })\n",
    "\n",
    "    for level in ['elementary', 'intermediate', 'advanced']:\n",
    "        level_items = [item for item in corpus if item['level'] == level]\n",
    "        correct = sum(1 for result in detailed_results if result['actual_level'] == level and result['actual_level'] == result['predicted_level'])\n",
    "        level_accuracy[level] = correct / len(level_items) if level_items else 0\n",
    "\n",
    "    weighted_correct = sum(1 * result['confidence'] for result in detailed_results if result['actual_level'] == result['predicted_level'])\n",
    "    total_weight = sum(result['confidence'] for result in detailed_results)\n",
    "    weighted_accuracy = weighted_correct / total_weight if total_weight > 0 else 0\n",
    "\n",
    "    return {\n",
    "        'correlation': correlation,\n",
    "        'overall_accuracy': accuracy,\n",
    "        'weighted_accuracy': weighted_accuracy,\n",
    "        'level_accuracy': level_accuracy,\n",
    "        'level_confusion': level_confusion,\n",
    "        'detailed_results': detailed_results\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    api = MoodleReadabilityAPI(\"http://192.168.178.108:8000\", \"de39a2339ba3cfe58a55e0711ca716d6\")\n",
    "    base_dir = \"/home/anrichp/Documents/Development/OneStopEnglishCorpus\"\n",
    "    \n",
    "    try:\n",
    "        corpus = read_corpus(base_dir)\n",
    "        accuracy_results = validate_accuracy(api, corpus)\n",
    "        \n",
    "        print(\"\\nAccuracy Validation Results:\")\n",
    "        print(f\"Overall Accuracy: {accuracy_results['overall_accuracy']:.2%}\")\n",
    "        print(f\"Weighted Accuracy: {accuracy_results['weighted_accuracy']:.2%}\")\n",
    "        print(f\"Correlation: {accuracy_results['correlation']:.2f}\")\n",
    "        \n",
    "        print(\"\\nAccuracy by Level:\")\n",
    "        for level, acc in accuracy_results['level_accuracy'].items():\n",
    "            print(f\"  {level.capitalize()}: {acc:.2%}\")\n",
    "\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        for actual_level, predictions in accuracy_results['level_confusion'].items():\n",
    "            print(f\"  {actual_level.capitalize()}:\")\n",
    "            for predicted_level, count in predictions.items():\n",
    "                print(f\"    Predicted as {predicted_level}: {count}\")\n",
    "\n",
    "        print(\"\\nDetailed Results:\")\n",
    "        for result in accuracy_results['detailed_results']:\n",
    "            print(f\"File: {result['file']}\")\n",
    "            print(f\"  Actual Level: {result['actual_level']}\")\n",
    "            print(f\"  Predicted Level: {result['predicted_level']}\")\n",
    "            print(f\"  Gunning Fog Index: {result['gunning_fog_index']:.2f}\")\n",
    "            print(f\"  Confidence: {result['confidence']:.2f}\")\n",
    "            print(f\"  Continuous Scale: {result['continuous_scale']:.2f}\")\n",
    "            print()\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
